{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ollama Setup and Testing\n",
        "\n",
        "This notebook will help you set up and test Ollama with LangChain connectors before starting the main RAG assignment.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "1. **Install Ollama** from https://ollama.ai\n",
        "   - On Linux/Mac: `curl https://ollama.ai/install.sh | sh`\n",
        "   - On Windows: Download and run the installer\n",
        "\n",
        "2. **Verify Installation**\n",
        "   - Run `ollama -v` in your terminal\n",
        "   - Should show version 0.11.10 or greater\n",
        "\n",
        "3. **Pull Required Models**\n",
        "   ```bash\n",
        "   # For chat/inference\n",
        "   ollama pull gpt-oss:20b\n",
        "   \n",
        "   # For embeddings\n",
        "   ollama pull embeddinggemma:latest\n",
        "   ```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Test Ollama Connection\n",
        "\n",
        "First, let's verify that Ollama is running and accessible:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Ollama is running!\n",
            "\n",
            "Available models:\n",
            "  - gpt-oss:20b\n",
            "  - embeddinggemma:latest\n",
            "  - llama2:13b\n",
            "  - llama2:latest\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# Test if Ollama is running\n",
        "try:\n",
        "    response = requests.get('http://localhost:11434/api/tags')\n",
        "    if response.status_code == 200:\n",
        "        models = json.loads(response.text)\n",
        "        print(\"✅ Ollama is running!\")\n",
        "        print(\"\\nAvailable models:\")\n",
        "        for model in models.get('models', []):\n",
        "            print(f\"  - {model['name']}\")\n",
        "    else:\n",
        "        print(\"❌ Ollama is not responding properly\")\n",
        "except requests.exceptions.ConnectionError:\n",
        "    print(\"❌ Cannot connect to Ollama. Make sure it's running!\")\n",
        "    print(\"Start Ollama by running 'ollama serve' in a terminal\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Test Embeddings with Ollama\n",
        "\n",
        "Now let's test creating embeddings using the LangChain Ollama connector:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Embedding model initialized\n"
          ]
        }
      ],
      "source": [
        "from langchain_ollama import OllamaEmbeddings\n",
        "\n",
        "# Initialize the embedding model\n",
        "embedding_model = OllamaEmbeddings(\n",
        "    model=\"embeddinggemma:latest\",\n",
        "    base_url=\"http://localhost:11434\"  # Default Ollama URL\n",
        ")\n",
        "\n",
        "print(\"✅ Embedding model initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding query: 'What is love? Baby don't hurt me, don't hurt me, no more.'\n",
            "\n",
            "✅ Successfully created embedding!\n",
            "Embedding dimension: 768\n",
            "First 10 values: [-0.122944966, 0.014476633, 0.016764784, -0.013229229, -0.04050047, -0.00504672, -0.01957866, 0.048353076, 0.003079206, -0.009721521]\n"
          ]
        }
      ],
      "source": [
        "# Test embedding a single query\n",
        "test_query = \"What is love? Baby don't hurt me, don't hurt me, no more.\"\n",
        "\n",
        "print(f\"Embedding query: '{test_query}'\")\n",
        "embedding = embedding_model.embed_query(test_query)\n",
        "\n",
        "print(f\"\\n✅ Successfully created embedding!\")\n",
        "print(f\"Embedding dimension: {len(embedding)}\")\n",
        "print(f\"First 10 values: {embedding[:10]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding multiple documents...\n",
            "\n",
            "✅ Successfully created 3 embeddings!\n",
            "\n",
            "Document 1: 'The quick brown fox jumps over the lazy dog....'\n",
            "  Embedding dimension: 768\n",
            "  First 5 values: [-0.14782572, 0.002899217, 0.05214841, -0.029081974, -0.036695857]\n",
            "\n",
            "Document 2: 'Machine learning is a subset of artificial intelli...'\n",
            "  Embedding dimension: 768\n",
            "  First 5 values: [-0.124072984, -0.0027437524, -0.0003292989, 0.010069011, 0.0016800934]\n",
            "\n",
            "Document 3: 'Python is a popular programming language for data ...'\n",
            "  Embedding dimension: 768\n",
            "  First 5 values: [-0.1626911, -0.010286673, 0.025470829, 0.0006976369, -0.01888673]\n"
          ]
        }
      ],
      "source": [
        "# Test embedding multiple documents\n",
        "test_documents = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"Machine learning is a subset of artificial intelligence.\",\n",
        "    \"Python is a popular programming language for data science.\"\n",
        "]\n",
        "\n",
        "print(\"Embedding multiple documents...\")\n",
        "embeddings = embedding_model.embed_documents(test_documents)\n",
        "\n",
        "print(f\"\\n✅ Successfully created {len(embeddings)} embeddings!\")\n",
        "for i, doc in enumerate(test_documents):\n",
        "    print(f\"\\nDocument {i+1}: '{doc[:50]}...'\")\n",
        "    print(f\"  Embedding dimension: {len(embeddings[i])}\")\n",
        "    print(f\"  First 5 values: {embeddings[i][:5]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Test Model Inference with Ollama\n",
        "\n",
        "Now let's test using Ollama for text generation/inference using the LangChain connector:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Chat model initialized\n"
          ]
        }
      ],
      "source": [
        "from langchain_ollama import ChatOllama\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "# Initialize the chat model\n",
        "chat_model = ChatOllama(\n",
        "    model=\"llama3.1:8b\",\n",
        "    temperature=0.6,\n",
        "    base_url=\"http://localhost:11434\",\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(\"✅ Chat model initialized\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#Let's add a procedure to measure the inference performance of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def detailed_performance_metrics(response_metadata):\n",
        "    \"\"\"\n",
        "    Calculate comprehensive performance metrics from Ollama response metadata\n",
        "    \"\"\"\n",
        "    # Extract all timing data (in nanoseconds)\n",
        "    total_duration = response_metadata.get('total_duration', 0)\n",
        "    load_duration = response_metadata.get('load_duration', 0)\n",
        "    prompt_eval_duration = response_metadata.get('prompt_eval_duration', 0)\n",
        "    eval_duration = response_metadata.get('eval_duration', 0)\n",
        "    \n",
        "    # Extract token counts\n",
        "    prompt_eval_count = response_metadata.get('prompt_eval_count', 0)\n",
        "    eval_count = response_metadata.get('eval_count', 0)\n",
        "    \n",
        "    # Convert to seconds\n",
        "    total_seconds = total_duration / 1_000_000_000\n",
        "    load_seconds = load_duration / 1_000_000_000\n",
        "    prompt_eval_seconds = prompt_eval_duration / 1_000_000_000\n",
        "    eval_seconds = eval_duration / 1_000_000_000\n",
        "    \n",
        "    # tokens per second\n",
        "    tokens_per_second = eval_count / eval_seconds\n",
        "\n",
        "    # Calculate metrics\n",
        "    metrics = {\n",
        "        'generation_tokens_per_second': eval_count / eval_seconds if eval_seconds > 0 else 0,\n",
        "        'prompt_tokens_per_second': prompt_eval_count / prompt_eval_seconds if prompt_eval_seconds > 0 else 0,\n",
        "        'total_tokens': prompt_eval_count + eval_count,\n",
        "        'total_time_seconds': total_seconds,\n",
        "        'load_time_seconds': load_seconds,\n",
        "        'generation_time_seconds': eval_seconds,\n",
        "        'prompt_processing_time_seconds': prompt_eval_seconds,\n",
        "        }\n",
        "\n",
        "    print(f\"Total tokens: {metrics['total_tokens']}\")\n",
        "    print(f\"Total time seconds: {metrics['total_time_seconds']}\")\n",
        "    print(f\"Load time seconds: {metrics['load_time_seconds']}\")\n",
        "    print(f\"Generation time seconds: {metrics['generation_time_seconds']}\")\n",
        "    print(f\"Prompt processing time seconds: {metrics['prompt_processing_time_seconds']}\")\n",
        "    print(f\"Generation tokens per second: {metrics['generation_tokens_per_second']}\")\n",
        "    print(f\"Prompt tokens per second: {metrics['prompt_tokens_per_second']}\")\n",
        "\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt: Explain quantum computing in one sentence.\n",
            "\n",
            "Generating response...\n",
            "\n",
            "✅ Response generated!\n",
            "\n",
            "Model output: Quantum computing uses the principles of quantum mechanics to perform calculations and operations on data at a scale that's exponentially faster than classical computers, allowing for complex problems to be solved that are currently unsolvable or require an unfeasible amount of time to solve with traditional computing methods.\n"
          ]
        }
      ],
      "source": [
        "# Test simple inference\n",
        "prompt = \"Explain quantum computing in one sentence.\"\n",
        "\n",
        "print(f\"Prompt: {prompt}\")\n",
        "print(\"\\nGenerating response...\")\n",
        "\n",
        "response = chat_model.invoke(prompt)\n",
        "\n",
        "print(f\"\\n✅ Response generated!\")\n",
        "print(f\"\\nModel output: {response.content}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total tokens: 74\n",
            "Total time seconds: 5.55217\n",
            "Load time seconds: 2.903604292\n",
            "Generation time seconds: 2.126368167\n",
            "Prompt processing time seconds: 0.519085333\n",
            "Generation tokens per second: 26.335984929179954\n",
            "Prompt tokens per second: 34.676379500015656\n"
          ]
        }
      ],
      "source": [
        "_ = detailed_performance_metrics(response.response_metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sending messages to model...\n",
            "\n",
            "✅ Response generated!\n",
            "\n",
            "Model output: Machine learning (ML) is a type of artificial intelligence (AI) that enables computers to learn from data without being explicitly programmed.\n",
            "\n",
            "Think of it like this: Imagine you're trying to teach a child how to recognize different animals. You wouldn't simply tell them \"this is a dog, this is a cat,\" and expect them to remember every detail. Instead, you'd show them many pictures of dogs and cats, and they would learn to recognize the patterns and characteristics that distinguish one from the other.\n",
            "\n",
            "Machine learning works in a similar way. It uses algorithms (step-by-step procedures) to analyze large datasets and identify patterns, relationships, or predictions. The more data it's exposed to, the better it becomes at making accurate decisions or predictions.\n",
            "\n",
            "There are three main types of machine learning:\n",
            "\n",
            "1. **Supervised Learning**: This is like teaching a child to recognize animals by showing them labeled pictures (e.g., \"this is a dog\"). The algorithm learns from these examples and can make predictions on new, unseen data.\n",
            "2. **Unsupervised Learning**: This is like letting the child explore a zoo with no labels or guidance. They would need to figure out what's what based solely on their observations. In machine learning, this type of learning involves identifying patterns or clustering similar items together without any prior knowledge.\n",
            "3. **Reinforcement Learning**: This is like teaching a child to play a game by giving them rewards for achieving certain goals (e.g., catching a ball). The algorithm learns through trial and error, adjusting its behavior based on feedback from the environment.\n",
            "\n",
            "Machine learning has many applications in various industries, such as:\n",
            "\n",
            "* Image recognition\n",
            "* Natural language processing (e.g., chatbots)\n",
            "* Predictive maintenance (e.g., detecting equipment failures)\n",
            "* Recommendation systems (e.g., personalized product suggestions)\n",
            "\n",
            "In summary, machine learning is a powerful tool that enables computers to learn from data and improve their performance over time, making it an essential component of modern AI.\n"
          ]
        }
      ],
      "source": [
        "# Test with system message and human message\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful AI assistant that explains complex topics simply.\"),\n",
        "    HumanMessage(content=\"What is machine learning?\")\n",
        "]\n",
        "\n",
        "print(\"Sending messages to model...\")\n",
        "response = chat_model.invoke(messages)\n",
        "\n",
        "print(f\"\\n✅ Response generated!\")\n",
        "print(f\"\\nModel output: {response.content}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total tokens: 444\n",
            "Total time seconds: 16.49599575\n",
            "Load time seconds: 0.078168959\n",
            "Generation time seconds: 16.154724541\n",
            "Prompt processing time seconds: 0.262441375\n",
            "Generation tokens per second: 25.50337512437068\n",
            "Prompt tokens per second: 121.93199338328418\n"
          ]
        }
      ],
      "source": [
        "_ = detailed_performance_metrics(response.response_metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Test Streaming Response\n",
        "\n",
        "Ollama supports streaming responses, which is useful for real-time applications:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt: Write a haiku about artificial intelligence.\n",
            "\n",
            "Streaming response:\n",
            "----------------------------------------\n",
            "Metal mind awakes\n",
            "Thinking, learning, growing fast\n",
            "Future's subtle form\n",
            "----------------------------------------\n",
            "\n",
            "✅ Streaming completed!\n"
          ]
        }
      ],
      "source": [
        "# Test streaming\n",
        "prompt = \"Write a haiku about artificial intelligence.\"\n",
        "\n",
        "print(f\"Prompt: {prompt}\")\n",
        "print(\"\\nStreaming response:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "for chunk in chat_model.stream(prompt):\n",
        "    print(chunk.content, end=\"\", flush=True)\n",
        "\n",
        "print(\"\\n\" + \"-\" * 40)\n",
        "print(\"\\n✅ Streaming completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "If all the tests above passed, you're ready to use Ollama with LangChain! Here's what we tested:\n",
        "\n",
        "✅ **Embeddings**: \n",
        "- Created embeddings for single queries\n",
        "- Created embeddings for multiple documents\n",
        "- Verified embedding dimensions\n",
        "\n",
        "✅ **Model Inference**:\n",
        "- Simple text generation\n",
        "- Chat with system and human messages\n",
        "- Streaming responses\n",
        "- Integration with LangChain chains\n",
        "\n",
        "## Troubleshooting\n",
        "\n",
        "If you encounter issues:\n",
        "\n",
        "1. **Model Not Found**: Pull the required models (`ollama pull <model-name>`)\n",
        "2. **Slow Performance**: Ollama models run on CPU by default. For better performance:\n",
        "   - Use smaller models for testing\n",
        "   - Consider GPU acceleration if available\n",
        "3. **Memory Issues**: Large models require significant RAM. Try smaller variants if needed.\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "Now you're ready to proceed with the main RAG assignment using Ollama!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
